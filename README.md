# hubert-pitdyn

    Try using HuBERT to encode pitch and dynamic information, towards pitdyn-guided Soft-VC.

----

âš  å‘ç°è¿™ä¼¼ä¹å¹¶æ²¡æœ‰ä»€ä¹ˆåµç”¨ ğŸ˜“ï¼Œæ”¾å¼ƒäº†ï¼ˆ  
âš  ä¸çŸ¥é“æ˜¯ä¸æ˜¯æˆ‘è®­ç»ƒ recipe æ²¡è°ƒå¥½ï¼Œhubert-pitdyn çš„å‡†ç¡®ç‡åªæœ‰ 50% å¤šä¸€ç‚¹ç‚¹ï¼Œè€Œä¸”ä¿®æ”¹ä¹‹åçš„å£°å­¦æ¨¡å‹è¿˜æ˜¯æ¥è¿‘ä» 0 å¼€å§‹è®­ç»ƒçš„  
âš  è¿™ä½¿å¾—åˆæˆçš„éŸ³é«˜é”™è¯¯å’Œå› éŸ³ç´ é”™è¯¯éƒ½æ¯”åŸå§‹çš„ hubert å¤šâ€¦â€¦  
âš  æ„Ÿè§‰è¿˜æ˜¯å¾—ä»æ›´åŠ åºå¤§çš„é¢„è®­ç»ƒçš„å‡å­¦æ¨¡å‹ä¸­å»å¾®è°ƒ ğŸ¤”ï¼Œä»¥é‡è®­ç»ƒhypernetworkæˆ–è€…embeddingçš„æ–¹å¼ :(  

In the original Soft-VC architecture, **HuBERT** is used for obtaining speech content while discarding vocal timbre.  It applies k-Means to cluster similar melspec frames all to one output, achieving these two goals above.  However, the melspec similarity is measured by Euclidian distance on spectrogram,  so that the same vowel in linguistic might be far enough under Euclidian distance, thus been divided in to different clusters.  In this case, **train dataset variety is crucial** to train a satisfactory model.  

Things get tough when you wants to model some timbre, but only has time-limited even pitch/dynamic-non-variant data (e.g. UATU voice bank recordings).  The original Soft-VC cannot handle this, synthesizing audio totally flat in pitch (robot-like).  Hence we try to introduce acoustic features (i.e. **pitch** and **dynamic**) along with HuBERT's hidden-units (which traditionally represents **content**).  

As a result, We again use HuBERT to train discrete pitch and dynamic embeddings, combining `content-units` and `acoustic-embeddings` to feed the downstream acoustic model.

### Quick Start

âšª Train your own vbank

The idea:

  - use standard dataset `DataBaker` to train hubert-pitdyn, note that a pitch/dynamic-variant dataset is necessary to learn the acoustic aprior. 
  - use voice bank recordings `UATU - ã¯ãªinit` to train acoustic-model, which is a pitch/dynamic-non-variant dataset. 
  - chain them up with pretrained HiFiGAN, now it's possible transfer any audio to timbre of  `ã¯ãªinit` with proper pitch & dynamic response.

#### train hubert-pitdyn

NOTE: you can use my pretrained weights on `DataBaker`, download from [here]() and put under `out\databaker` folder

- download dataset `DataBaker` or `LJSpeech`
- train hubert-pitdyn on a large dataset (e.g.: databaker)
  - preprocess with `preprocess_hubert-pitdyn.cmd <dataset> <path\to\wavs>`
    - manually modify `class hp` in 'audio.py', set `st_*` fields according to the generated file 'data\<dataset>\stats.txt' (`f0` means pit, `c0` means dyn)
  - train pit model with `python train_hubert.py --embds_dn pits <preprocessed_path> <log_path>`
  - train dyn model with `python train_hubert.py --embds_dn dyns <preprocessed_path> <log_path>`

#### train acoustic model

NOTE: this routine is nearly the same as in [soft-vc-acoustic-models](https://github.com/Kahsolt/soft-vc-acoustic-models)

- train acoustic model on your small target timbre dataset (e.g.: hana)
  - preprocess with `preprocess_acoustic.cmd <vbank> <path\to\wavs> <dataset>`
  - train with `python train_acoustic.py <vbank> <log_path>`

Here's a full concrete example:

```powershell
# Step 1: train hubert-pitdyn
# <dataset>=databaker
# <path\to\wavs>=C:\Data\BZNSYP\Wave
preprocess_hubert-pitdyn.cmd databaker C:\Data\BZNSYP\Wave

# <preprocessed_path>=data\<dataset>
# <log_path>=out\<dataset>\hubert-[pit|dyn]
python train_hubert.py --embds_dn pits data\databaker out\databaker\hubert-pit
python train_hubert.py --embds_dn dyns data\databaker out\databaker\hubert-dyn

# Step 2: pretrain acoustic model on a larger dataset
# <vbank>=databaker
# <path\to\wavs>=C:\Data\databaker\wavs
# <dataset>=databaker
preprocess_acoustic.cmd databaker C:\Data\BZNSYP\Wave databaker

# <vbank>=databaker
# <log_path>=out\<vbank>\acoustic
python train_acoustic.py data\databaker out\databaker\acoustic

# Step 3: refine that acoustic model on your small dataset
# <vbank>=hana
# <path\to\wavs>=C:\Data\hana\wavs
# <dataset>=databaker
preprocess_acoustic.cmd hana C:\Data\hana databaker

# <vbank>=hana
# <log_path>=out\<vbank>\acoustic
python train_acoustic.py data\hana out\hana\acoustic --resume out\databaker\acoustic\model-best.pt --refine

# Step 4: now you can infer
# <vbank>=hana
# <dataset>=databaker
# => generated under 'gen' folder
python infer.py hana --dataset databaker --input test\BAC009S0724W0121.wav
```

### Project Layout

```
.
â”œâ”€â”€ thesis/                   // å‚è€ƒç”¨åŸå§‹è®ºæ–‡
â”œâ”€â”€ hubert_pitdyn/            // å£°å­¦ç¼–ç å™¨æ¨¡å‹ä»£ç 
â”œâ”€â”€ acoustic/                 // å£°å­¦æ¨¡å‹ä»£ç 
â”œâ”€â”€ data/                     // è®­ç»ƒç”¨æ•°æ®æ–‡ä»¶
â”‚Â Â  â”œâ”€â”€ <vbank>/
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ wavs/             // æŒ‡å‘<wavpath>çš„ç›®å½•è½¯è¿æ¥ (ç”±mklinkäº§ç”Ÿ)
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ units/            // preprocessäº§ç”Ÿçš„HuBERTç‰¹å¾
â”‚Â Â  â”‚Â Â  â””â”€â”€ mels/             // preprocessäº§ç”Ÿçš„Melè°±ç‰¹å¾
â”‚Â Â  â””â”€â”€ ...
â”œâ”€â”€ out/                      // æ¨¡å‹æƒé‡ä¿å­˜ç‚¹ + æ—¥å¿—ç»Ÿè®¡
â”‚Â Â  â”œâ”€â”€ <vbank>/
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ logs/             // æ—¥å¿—(`*.log`) + TFBoard(`events.out.tfevents.*`)
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ model-best.pt     // æœ€ä¼˜æ£€æŸ¥ç‚¹
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ model-<steps>.pt  // ä¸­é—´æ£€æŸ¥ç‚¹
â”‚Â Â  â””â”€â”€ ...
â”œâ”€â”€ preprocess.py             // æ•°æ®é¢„å¤„ç†ä»£ç 
â”œâ”€â”€ train.py                  // è®­ç»ƒä»£ç 
â”œâ”€â”€ infer.py                  // åˆæˆä»£ç  (Commandline API)
â”œâ”€â”€ demo.ipynb                // ç¼–ç¨‹APIç¤ºä¾‹ (Programmatic API)
|â”€â”€ ...
â”œâ”€â”€ make_train.cmd            // é¢„å¤„ç†è„šæœ¬ (ä»…é¢„å¤„ç†ï¼Œæ­¥éª¤1~3)
â”œâ”€â”€ make_preprocess.cmd       // è®­ç»ƒè„šæœ¬ (ä»…è®­ç»ƒï¼Œæ­¥éª¤4)
|â”€â”€ ...
â”œâ”€â”€ test/                     // demoæºæ•°æ®é›†
â”œâ”€â”€ gen/                      // demoç”Ÿæˆæ•°æ®é›† (demoæºæ•°æ®é›†åœ¨demoå£°åº“ä¸Šäº§ç”Ÿçš„è½¬æ¢ç»“æœ)
â”œâ”€â”€ index.html                // demoåˆ—è¡¨é¡µé¢
â”œâ”€â”€ make_index.py             // demoé¡µé¢ç”Ÿæˆè„šæœ¬ (äº§ç”Ÿindex.html)
â””â”€â”€ make_infer_test.cmd       // demoç”Ÿæˆæ•°æ®é›†ç”Ÿæˆè„šæœ¬ (äº§ç”Ÿgen/)
```

â„¹ï¸ These developed scripts and tools are targeted mainly for **Windows** platform, if you work on Linux or Mac, you possibly need to modify on your own :(

### References

Great thanks to the founding authors of Soft-VC! :lollipop:

```
@inproceedings{
  soft-vc-2022,
  author={van Niekerk, Benjamin and Carbonneau, Marc-AndrÃ© and ZaÃ¯di, Julian and Baas, Matthew and SeutÃ©, Hugo and Kamper, Herman},
  booktitle={ICASSP}, 
  title={A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion}, 
  year={2022}
}
```

- soft-vc paper: [https://ieeexplore.ieee.org/abstract/document/9746484](https://ieeexplore.ieee.org/abstract/document/9746484)
- hubert: [https://github.com/bshall/hubert](https://github.com/bshall/hubert)

----

by Armit
2022/09/17 
